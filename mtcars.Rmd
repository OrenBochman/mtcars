---
title: "Mtcars's story in pictures and the numbers"
output: html_notebook
---

```{r}

require(stats);
require(graphics)
```

# background

MTCARS or for "Motor Trend Car Road Tests"  is a data set extracted from  March, April, June and July issues of 1974 [Motor Trend](http://www.motortrend.com/) US magazine. This example of multivariate data had been suggested by Dr. R. J. Freund, Institute of Statistics, Texas A&M University to Dr Ronald R. Hocking  known for "Methods and Applications of Linear Models : Regression and the Analysis of Variance"
for an invited paper to Biometrics journal. The paper titled "A Biometrics Invited Paper. The Analysis and Selection of Variables in Linear Regression"
with 10 features for 32 cars models. It is included in r datasets package.

the features are cryptically coded as follow:

y   [, 1]	 mpg	 Miles/(US) gallon
x1  [, 8]	 vs	   V/S originally Engine Shape (Straight (1) or V(O))
x2  [, 2]	 cyl	 Number of cylinders
x3  [, 9]	 am	   Transmission type (0 = automatic, 1 = manual) - was original (Manual (1) or Auto (C))
x4  [,10]	 gear	 Number of forward gears original X4 Number of Transmission Speeds
x5  [, 3]	 disp	 Displacement (cu.in.) originally  Engine Size (Cubic Inches)
x6  [, 4]	 hp	   Gross horsepower
x7  [,11]	 carb	 Number of carburetor barrels
x8  [, 5]	 drat	 Rear axle ratio originally (Final Drive Ratio)
x9  [, 6]	 wt	   Weight (1000 lbs)
x10 [, 7]	 qsec	 1/4 mile time in seconds



Analysis was done on a 1970 version of SELECT, a FS program, a BE program and, as a check, all possible regressions were evaluated. I.e. three programs were used. Hocking reports that the best model used x3 (am) , x9 (wt) and x10 (qsec) which was the model I arrived at. There is something of a challance here as these are not the best predictors individually and provide an advantage when used together.

The dataset was mistakenly credited to Henderson and Velleman (1981), "Building multiple regression models interactively". *Biometrics*, **37**, 391â€“411. However, Henderson and Velleman, were responding to a much longer invited paper by Hocking (1976). Considering the paper by Hocking was 50 pages long - with 6 pages of citations, it must have taken a while to create the data set, analyse and write it up. I fund it rather quaint that these invited papers were published in an established paper on Biometrics were using car data which suggests that looking at car data might be have more of a universal appeal than any data set in their own domain.

Hocking (1976) discusses at length the issue of feature selection and how to go about finding an subset for producing the optimal model. The focus is goodness of fit ad not predictive power. Also since it is regression most of the discussion is based on analytic motivation and ridge regression (i.e. regression with wight decay) is considered. Step-wise methods are also criticized

In the abstract the authors are offering a modern look at data science stating how automated tools fail to consider features such as non-linearity, col-linearity, outliers, and points with high leverage which can dramatically affect automated analyses yet remain undetected. They recommend a exploratory approach using a interactive software package rather than one that provides an automated solution. 

Our approach contrasts with such automated procedures as step-wise, forward selection, backwards elimination, best subsets and principal components regression. 

All this is fairly interesting because mtcars is possibly one of the first multidimensional datasets encountered by students of multiple regression. There are many student projects based on mtcars and even intrepid medium articles claiming to make a serious analysis at this dataset.
```{r}
cols <- colnames(mtcars)

# collumns and thier types
apply(mtcars,2,typeof)
```

nothing too exciting - all are double so no special treatment of nominal vars though "am" is the transmission type is a categorical variable.

# inspect the data
```{r exploratory basic}
head(mtcars)
```


# summary statistics

## using summary




```{r}
#print(summary(mtcars, digits=2))
ss <- apply(mtcars,2,summary)
#typeof(ss)
ss
```

## using describe

```{r}
library(psych)
#describe(mtcars)
```

# using Tukey five

```{r}
apply(mtcars,2 ,fivenum)
```

# Histograms

```{r eval=FALSE}
#apply(mtcars,2,histogram)
```
# ggplot2 histograms

```{r, fig.width=12,fig.height=12}
library(ggplot2)
library(reshape2)
ggplot(data = melt(mtcars), mapping = aes(x = value)) + 
    geom_histogram(bins = 10) + facet_wrap(~variable, scales = 'free_x')
```

# interactions of pairs

## correlations
```{r}
correlations <- cor(mtcars)
print(correlations,digits = 3)

```
to get at the eigen values of the matrix 

```{r}
#tricorr <- lower.tri(correlations)
ev <- eigen(correlations);
print(ev$values)
```


if we also want to get the p values we need to do this:
```{r}
library("Hmisc")
correlations2 <- rcorr(as.matrix(mtcars))
correlations2

```
lets flatten that

```{r}
# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

library(Hmisc)
correlations3<-rcorr(as.matrix(mtcars[,1:10]))
flattenCorrMatrix(correlations3$r, correlations3$P)
```

```{r}
library(corrplot)
corrplot(correlations, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

## pair plots

```{r pairs,fig.width=12,fig.height=12}

pairs(mtcars, main = "mtcars data")

```
```{r,fig.width=12,fig.height=12}
library("PerformanceAnalytics")
#my_data <- mtcars[, c(1,3,4,5,6,7)]
chart.Correlation(mtcars, histogram=TRUE, pch=19)
```
this chart shows:
* variable distribution (diagonal),
* the bi-variate scatter plots + fit (blow diagonal)
* the correlation coefficient with its significance marked schematically (above diagonal)


# N-Way Frequency Table 

```{r}
y = ftable(xtabs(formula = ~  gear + cyl + carb , data=mtcars))
y
```
this is not very useful data - however as it relay applies to categorical data

# Regression

```{r}
# Multiple Linear Regression with coefficents ordered by t value (signal to noise)
fit <- lm(mpg ~ wt + am + qsec + hp +  disp  + drat + gear + carb + vs + cyl,  data=mtcars)
summary(fit) # show results
```
## Assessing Outliers

```{r}
library(car)

outlierTest(fit) # Bonferonni p-value for most extreme obs
```
```{r}

qqPlot(fit, main="QQ Plot") #qq plot for studentized resid 
```
## Leverage plots
```{r levrage plots, fig.height=12,fig.retina=TRUE}
#par(mcol=c(2,2)) 
#l=layout(matrix(c(1,2,3,4,5,6,7,8,9,10), 4,4, byrow = TRUE))
leveragePlots(fit,layout=c(2,5)) # leverage plots
```


## Diagnostic plots

```{r fig.width=12,fig.height=12}
# diagnostic plots 
par(mfrow=c(2,2))
#layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(fit)
```

notes:

"residual plots may suggest transformations and also may reveal bad data points or "outliers."

* residuals / fitted values - 
* std residual / theoretical quantiles
* sqrt(std residual) / fitted value
* std residual / leverage


## coefficents

```{r}
# Other useful functions 
coefficients(fit) # model coefficients
``` 

## confidence intervals



```{r}
confint(fit, level=0.95) # CIs for model parameters 

```


## predicted values

```{r}

fitted(fit) # predicted values
```


## residuals
```{r}

residuals(fit) # residuals
```
## anova table 
```{r}

anova(fit) # anova table 
```

## covariance matrix for model parameters 
```{r}

vcov(fit) # covariance matrix for model parameters 
```

```{r}

influence(fit) # regression diagnostics
```
## regression diagnostics

```{r coplot, fig.width=12}
coplot(mpg ~ disp | as.factor(cyl), data = mtcars,
       panel = panel.smooth, rows = 1)

```

# Stepwise Regression

```{r stepwiseregression}
library(MASS)

step <- stepAIC(fit, direction="both")
step$anova # display results

```
conclusion - best fit model is with just wt, am and qsec
wt - (weight )
am - (transmission)
qsec - (1/4 mile time)

```{r}
d <- dist(as.matrix(mtcars))   # find distance matrix 
hc <- hclust(d)                # apply hirarchical clustering 
plot(hc) 
```

# References

1. Harold V. Henderson and Paul F. Velleman [Building Multiple Regression Models Interactively](http://www.jstor.org/stable/2530428) Biometrics, Vol. 37, No. 2 (Jun., 1981), pp. 391-411 International Biometric Society Accessed: 28/04/2019 08:51

2. R. R. Hocking [A Biometrics Invited Paper. The Analysis and Selection of Variables in Linear Regression](http://www.jstor.org/stable/2529336) Biometrics, Vol. 32, No. 1 (Mar., 1976), pp. 1-49 International Biometric Society Accessed: 28/06/2014 08:27
2. [Motor Trend](http://www.motortrend.com/) 1974